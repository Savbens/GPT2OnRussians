# GPT2OnRussians
Goal of project is try to train GPT2 model on auto-translated Tiny Stories dataset

# Генерация текста с использованием трансформеров

Этот проект реализует систему генерации текста на основе дообученной модели GPT-2, способной создавать продолжения текста на английском и русском языках. Система использует датасет TinyStories, дополненный переводами на русский язык, и применяет токенизатор и модель GPT-2 для генерации текста.

## Обзор проекта

Цель проекта — создание системы генерации текста, способной создавать логичные продолжения входных текстов на английском и русском языках. Система основана на архитектуре трансформеров, в частности, на модели GPT-2, дообученной на двуязычном датасете. Проект включает подготовку данных, дообучение модели, генерацию текста и оценку качества результатов.

## Возможности

- **Двуязычная генерация текста**: Создание продолжений текста на английском и русском языках на основе входных фраз.
- **Дообученная модель GPT-2**: Используется предобученная модель GPT-2, дообученная на датасете TinyStories с русскими переводами.
- **Механизм внимания**: Применяется механизм scaled dot-product attention для обработки контекста текста.
- **Оценка качества**: Используются метрика perplexity и качественные критерии (грамматическая корректность, смысловая связность, языковая стабильность и креативность) для оценки сгенерированного текста.

## Структура проекта

- **Теоретическая часть**:
  - Обзор архитектуры трансформеров (структура энкодер-декодер, многослойное само-внимание).
  - Описание механизма scaled dot-product attention.
- **Практическая часть**:
  - **Подготовка данных**: Используется датасет `translated_tiny_stories.jsonl`, содержащий пары текстов на английском и русском языках. Класс `TextGenerationDataset` отвечает за загрузку и токенизацию данных с помощью `GPT2Tokenizer`.
  - **Дообучение модели**: Дообучение модели `GPT2LMHeadModel` с использованием оптимизатора AdamW на подготовленных данных.
  - **Генерация текста**: Реализована функция `generate_continuation` с параметрами `max_length=256`, `temperature=0.7`, `top_k=50`, `top_p=0.95` и `no_repeat_ngram_size=2` для генерации связного текста.
  - **Оценка**: Оценка сгенерированного текста с использованием perplexity (английский: ~3.48, русский: ~9.51) и качественных критериев.

## Результаты

- **Генерация текста на английском языке**:
  - Высокое качество текстов с хорошей грамматической корректностью и смысловой связностью.
  - Значения perplexity от 3.08 до 3.93, что указывает на высокую уверенность модели.
  - Подходит для нарративных задач, с креативными и интересными сюжетами.
- **Генерация текста на русском языке**:
  - Низкое качество из-за недостаточного объема или качества русских данных, а также шума в датасете.
  - Значения perplexity от 6.15 до 11.91, что отражает меньшую уверенность модели.
  - Проблемы включают грамматические ошибки, семантические несоответствия и переключение на английский язык.

## Установка

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/yourusername/text-generation-transformers.git
   ```
2. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```
   Требуемые библиотеки включают `torch`, `transformers` и другие, указанные в `requirements.txt`.
3. Скачайте датасет `translated_tiny_stories.jsonl` или подготовьте собственный двуязычный датасет.

## Использование

1. **Подготовка данных**:
   - Убедитесь, что датасет в формате `translated_tiny_stories.jsonl` содержит пары текстов на английском и русском языках.
   - Запустите скрипт предобработки данных для токенизации:
     ```bash
     python preprocess_data.py
     ```

2. **Дообучение модели**:
   - Запустите скрипт дообучения:
     ```bash
     python finetune_model.py
     ```

3. **Генерация текста**:
   - Используйте скрипт генерации текста с входной фразой:
     ```bash
     python generate_text.py --prompt "Once upon a time" --language "en"
     python generate_text.py --prompt "Жила-была девочка" --language "ru"
     ```

4. **Оценка результатов**:
   - Запустите скрипт оценки для вычисления perplexity и качественных метрик:
     ```bash
     python evaluate.py
     ```

## Возможные улучшения

- **Улучшение качества русских текстов**:
  - Использование отдельного токенизатора или модели для русского языка.
  - Расширение датасета русскоязычных текстов для повышения качества обучения.
  - Применение подходов с переводом (например, перевод входного текста на английский, генерация, затем обратный перевод).
- **Оптимизация обучения**:
  - Эксперименты с настройкой гиперпараметров для улучшения производительности модели.
  - Устранение шума в датасете для сокращения ошибок в генерации русских текстов.
- **Расширение оценки**:
  - Включение дополнительных метрик, таких как BLEU или ROUGE, для более полной оценки.

## Благодарности

- Проект основан на архитектуре трансформеров, представленной в статье *"Attention Is All You Need"* (Васвани и др., 2017).
- Используется библиотека `transformers` от Hugging Face для работы с моделью и токенизатором GPT-2.
- Датасет TinyStories служит основой для обучающих данных.